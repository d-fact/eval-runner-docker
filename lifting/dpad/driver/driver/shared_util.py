from pathlib import Path
from typing import List, Tuple
import logging
import time
import shlex
import json
from util import check_dir, log_popen
from functools import partial
from multiprocessing import Pool
import doop_util
from build import step_compile
from run_config import DirsTuple, RunConfig, DoopCfg
from commits_util import interp_rev_range
from git import Commit, Repo
from subprocess import Popen, PIPE

logger = logging.getLogger(__name__)


def store_stat(stat_dir: Path, time_dict: dict):
    with open(str(stat_dir / "step-time.json"), 'w') as sf:
        json.dump(time_dict, sf, indent=2)


def build_step(incr: bool, cfg: RunConfig, repo: Repo, time_dict: dict) -> List[str]:
    """
    Build jar for multiple versions of a repo
    :param incr: incremental build or not
    :param cfg: all configurations read from *.toml
    :param repo: a GitPython git.Repo object
    :param time_dict: recording running time
    :return: the list of commits (sha1 strings)
    """
    outdirs: DirsTuple = cfg.out_dirs
    check_dir(str(outdirs.jar), make_if_not=True)
    commit_list: List[str] = interp_rev_range(repo, cfg.rev_range)
    commit_list_repr = '\n'.join(commit_list)
    logger.debug(f"Commits to be processed are:\n{commit_list_repr}")
    start_time: float = time.time()
    if incr:
        logger.debug("Build mode: Incremental build")
        step_compile(outdirs.jar, repo, commit_list, cfg.njobs, True)
    else:  # default
        logger.debug("Build mode: Non-incremental build")
        step_compile(outdirs.jar, repo, commit_list, cfg.njobs, False)

    time_dict["compile"] = time.time() - start_time
    return commit_list


def collect_step(cfg: RunConfig, time_dict: dict):
    """
    Run doop for each subdir of generated bytecode
    :param cfg: all configurations
    :param time_dict: recording running time
    """
    outdirs: DirsTuple = cfg.out_dirs
    doop_cfg: DoopCfg = cfg.doop
    check_dir(str(outdirs.facts), make_if_not=True)
    jar_list = []  # type: List[Path]
    for x in outdirs.jar.iterdir():
        if not x.is_file() or x.suffix != '.jar':
            logger.warning(f"Skip non-jar entry: {x}")
            continue
        else:
            logger.info(f"Add {x}.")
            jar_list.append(x)
    if doop_cfg.main_class:
        specify_main = f"--main {doop_cfg.main_class}"
    else:
        specify_main = ""
    partial_worker = partial(doop_util.facts_collect_worker, specify_main, doop_cfg.sha1digit, doop_cfg.annotate,
                             cfg.project_name, outdirs.facts, cfg.log_level, doop_cfg.atype, doop_cfg.path, outdirs.log)
    start_time: float = time.time()
    with Pool(processes=cfg.njobs) as p:
        ret = p.map(partial_worker, jar_list)
    time_dict["collect"] = time.time() - start_time


def merge_facts(facts_storage: Path, dest: Path, rm_dup: bool, merge_bin: Path, log_dir: Path) -> bool:
    """
    Merge annotated facts
    :param facts_storage: path to facts
    :param dest: path to merged facts
    :param rm_dup: remove duplicates or not
    :param merge_bin: binary for doing the actual merge
    :param log_dir: path to logs
    :return: False if stderr is not empty, True otherwise
    """
    subdirs: str = " ".join((str(x) for x in facts_storage.iterdir()))
    rmdup_opt: str = "--rm-dup" if rm_dup else ""
    cmd_merge: str = f"{merge_bin} -m {dest} {rmdup_opt} -r -- {subdirs}"
    logger.debug(f'Execute: {cmd_merge}')
    p = Popen(shlex.split(cmd_merge), stdout=PIPE, stderr=PIPE)
    log_dir = log_dir / "sdiff-merge"
    return log_popen(p, log_dir)


def facts_diff_ddlog(facts_storage: Path, diff_bin: Path, log_dir: Path) -> bool:
    """
    Do consecutive diff on facts of n versions,
    different with merge, we need (n-1) diff for n versions
    :param facts_storage: paths to facts
    :param diff_bin: binary for doing the actual diff generation
    :param log_dir: path to logs
    :return: False if diff_bin produce non-empty stderr on any two versions
    """
    subdirs: List[Path] = list(facts_storage.iterdir())
    flag: bool = True
    for idx in range(len(subdirs) - 1):
        consec: Tuple[Path, Path] = (subdirs[idx], subdirs[idx + 1])
        log_path = log_dir / "ddlog-diff" / f"{consec[0].stem[:8]}-{consec[1].stem[:8]}"
        flag = flag and _facts_diff_ddlog_worker(consec, diff_bin, log_path)
    return flag


def _facts_diff_ddlog_worker(dirs: Tuple[Path, Path], diff_bin: Path, log_dir: Path) -> bool:
    """
    Call diff binary to generate ddlog-style insert/delete statements for two versions
    :param dirs: path to two versions of facts
    :param diff_bin: binary for doing the actual diff generation
    :param log_dir: path to logs
    :return: False if stderr is not empty, True otherwise
    """
    two_dirs: str = " ".join((str(x) for x in dirs))
    cmd_diff: str = f"{diff_bin} -db -r -- {two_dirs}"
    logger.debug(f'Execute: {cmd_diff}')
    p = Popen(shlex.split(cmd_diff), stdout=PIPE, stderr=PIPE)
    return log_popen(p, log_dir)


def ddlog_analysis(cfg: RunConfig) -> bool:
    input_facts = cfg.out_dirs.ddlog / "converted.dat"
    output =  cfg.out_dirs.ddlog / "analysed.dump"
    cmd: str = f'{cfg.ddlog_pe_bin} < {input_facts} > {output}'
    logger.debug(f'Execute: {cmd} (with input from {input_facts} and output at {output})')
    p = Popen(cmd, shell=True)
    # with input_facts.open('rb', 0) as inpf, output.open('w') as outf:
    #     p = Popen(shlex.split(cmd), stdin=inpf, stdout=PIPE, stderr=PIPE)
    #     log_dir = cfg.out_dirs.log / "ddlog"
    #     return log_popen(p, log_dir)
    # return False
